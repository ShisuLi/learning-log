# 6 半监督学习与图正则

## 6.1 基本设置与假设
- 数据：少量有标签 $\{(x^r,\hat y^r)\}_{r=1}^R$，大量无标签 $\{x^u\}_{u=R+1}^{R+U}$，常见 $U\gg R$。可分直推式（无标签即测试集）与归纳式。
- 核心假设：**平滑**（近邻同标签）、**聚类**（决策边界穿过低密度区）、**流形**（高维数据位于低维流形）。

## 6.2 生成式半监督（EM）
- 假设类别条件分布（如共享协方差的高斯），参数 $\theta=\{P(C_i),\mu_i,\Sigma\}$。
- **E 步**：$P(C_i|x^u)=\frac{P(x^u|C_i)P(C_i)}{\sum_j P(x^u|C_j)P(C_j)}$。
- **M 步**：用真标签+软标签重估先验与均值/协方差，例如
  $$
  P(C_i)=\frac{\sum_{x^r\in C_i}1+\sum_{u}P(C_i|x^u)}{R+U},\quad
  \mu_i=\frac{\sum_{x^r\in C_i}x^r+\sum_u P(C_i|x^u)x^u}{\sum_{x^r\in C_i}1+\sum_u P(C_i|x^u)}
  $$
- 迭代直至收敛，初始化影响大。

## 6.3 低密度分离方法
- **自训练**：用当前模型给无标签打伪标签，挑高置信样本并加入训练集；风险为错误累积，可配合软标签/权重。
- **熵正则**：在损失中加入 $\lambda\sum_u -\sum_c P(y=c|x^u)\log P(y=c|x^u)$，鼓励低熵预测。
- **S3VM**：枚举无标签的标签赋值以最大化间隔，计算代价高，多用近似。

## 6.4 平滑假设与图方法
- **聚类后标注**：先无监督聚类，再用少量标签投票赋予簇标签。
- **图半监督**：
  - 构图：节点为样本，边权 $s(x_i,x_j)$（KNN 或 $\varepsilon$ 邻域，RBF $e^{-\|x_i-x_j\|^2/2\sigma^2}$）。
  - 平滑度：$S=\frac12\sum_{i,j}s(x_i,x_j)(y_i-y_j)^2=Y^\top L Y$，$L=D-W$ 为图拉普拉斯。
  - 总损失：$L=\sum_{r}C(y^r,\hat y^r)+\lambda S$，标签通过图传播。

## 6.5 现代深度 SSL 概览
- **一致性正则**：对同一无标签样本的不同增广输出需一致（如 MSE/KL），逼迫模型学习不变特征。
- **MixMatch**：多次增广平均→锐化伪标签→MixUp 混合有/无标签样本；无标签用 MSE。
- **FixMatch**：弱增广得高置信伪标签，强增广匹配该标签，门限控制噪声；简单高效基线。
- **对比学习预训练**（SimCLR/MoCo）+ 少量标注微调，可视作两阶段 SSL。
