# 4 预训练语言模型与大模型现象

## 4.1 自监督语言建模范式
- **GPT（自回归 LM）**：预测下一个 token；在海量文本上学语法/语义/世界知识。支持零样本/单样本/少样本 **上下文学习**（In-context Learning）。
- **BERT（去噪自编码器）**：
  - **MLM**：15% token Mask/随机替换/保留，预测原词，实现深度双向表示。
  - **NSP→SOP**：原版判断句子是否相邻，效果有限；SOP 交换相邻片段顺序，更关注语篇连贯。
  - 特殊符号 `[CLS]/[SEP]`；输出可微调用于多种下游任务（GLUE 等）。
- **多语 BERT**：在 104 语言预训练，自动对齐跨语义空间，可迁移到低资源语言。
- **T5/BART/MASS**：统一 Text-to-Text 预训练；多种破坏-还原任务；C4 语料支撑。

## 4.2 大模型 Scaling 与涌现
- **模型族与规模里程碑**：ELMo(94M)→BERT(340M)→GPT-2(1.5B)→T5(11B)→GPT-3(175B)→Switch Transformer(1.6T) 等。
- **涌现能力**：算术、CoT、多步推理等在规模跨临界点后突然提升。
- **校准性**：更大模型置信度更匹配真实正确率。
- **逆缩放/U 型任务**：部分任务随规模先降后升或持续下降，暴露模型易受上下文噪声干扰。
- **MoE（Switch Transformer）**：路由 Token 至单个专家，保持计算量不变而扩张总参数。

## 4.3 数据工程与训练要点
- **数据处理**：内容过滤、文本抽取、质量/重复检测、测试集污染过滤，C4 等清洗语料。
- **样本配置**：小模型可用大数据，大模型反而需高质量小数据以保证性价比。
- **KNN-LM/检索增强**：以检索近邻补充稀有知识。

## 4.4 GPT 与自监督在多模态/语音
- 笔记涵盖语音/图像 SSL 代表（SimCLR、BYOL）；理念相同：构造缺失/增广预测任务，学鲁棒表示。
