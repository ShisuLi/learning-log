# 7 历史脉络与 Scaling

## 7.1 深度学习里程碑
- 1958 感知机 → 1969 感知机局限（XOR） → 1980s 反向传播、多层感知机 → 2006 RBM 预训练 → 2009 GPU 加速 → 2011 语音突破 → 2012 AlexNet ImageNet 夺冠。
- 图像模型演进：AlexNet(8 层,16.4%) → VGG(19 层,7.3%) → GoogleNet(22 层,6.7%) → ResNet(152 层,3.57%)。
- 现代大模型：BERT/GPT 家族、T5、Switch Transformer 等进入百亿/万亿参数时代。

## 7.2 Scaling 现象
- **涌现能力**：规模跨阈值后解锁算术、多步推理、CoT 等新能力。
- **校准**：大模型置信度更匹配正确率。
- **逆缩放/U 型任务**：部分任务随规模先降后升或单调下降，提示上下文噪声敏感性。
- **MoE**：Mixture-of-Experts（如 Switch Transformer）用路由选择少量专家参与前向，扩容参数同时控制计算。

## 7.3 数据与工程
- **数据管线**：内容过滤、提取、质量筛选、去重、测试集泄漏防护；C4 等清洗语料为大模型提供基座。
- **训练策略**：小模型可用更大更杂的数据，大模型强调高质样本；KNN-LM/检索增强缓解长尾知识。

## 7.4 生成式浪潮
- 图文音生成范式：自回归文本、潜变量图像（VAE/Flow/Diffusion/GAN）、MoE 与多模态扩展。
- Emergent & Scaling Law 说明“量变→质变”，但逆缩放提醒需谨慎评估不同任务。
