# Diffusion

- 从Normal Distribution中sample一个vector，和我们想要的图片维度一致
- 噪声输入Denoise model不断去噪，reverse process
The sculpture is already complete within the marble block, before I start my work. It is already there, I just have to chisel away the superfluous material. — Michelangelo
- 过程中同一个denoise model，但是和图片同时输入的每一步的time step不同，代表当前噪声的程度
- Denoise模块：Noise predictor，输入图片和time step，输出当前图片的noise，再用输入图片减去这个预测的noise，得到output图片；产生noise比直接产生denoise结果更简单
- 训练资料是人为创建的，即forward process，也是diffusion process：从一张图片开始，不断加入noise，直到变成纯噪声；每一步就是训练数据，输入是当前有noise的图片和time step，输出ground truth是当前加入的noise

### Text-to-Image Generator
LAION 5.85B数据的图片，ImageNet 1M，需要文字和影像成对的资料；Noise predictor再加一个文字输入

### Stable Diffusion
- Text Encoder：把文字编码成向量
- Generation Model: 输入噪音和文字向量，得到中间产物，代表图片被压缩的结果；训练需要成对数据;噪音逐步加在latent representation上而不是图片上；训练Noise predictor，输入step数、文字Encoder结果、latent representation with noise，输出当前的noise
- Decoder：把中间产物还原出图片，训练不需要成对数据；中间产物是小图，就训练模型把小图还原成大图；中间产物是laten representation，训练Auto-encoder，拿出decoder使用；H*W*3-Encoder-h*w*c-Decoder

DALLE：Text Encoder + Autoregressive/Diffusion + Decoder


Encoder影响效果，越大越好，FID-10K分数越低
Decoder：不同大小的diffusion中的 noise predictor不太影响结果

Frechet Inception Distance (FID) score: 评估生成图片和真实图片的差异，分数越低越好
- 预先训练好CNN，对图像生成latent representation
- 计算生成图片和真实图片的latent representation的差异
- 假设两个distribution都是Gaussian distribution，计算两个distribution的Frechet Distance
- 需要sample大量的图片才能计算FID


### CLIP， Contrastive Language-Image Pre-Training
- Text Encoder + Image Encoder
- 用400 million对文字和图片做训练
- 成对的文字和图片的embedding要接近，不成对的要远离
- 输入生成的图片和指示的文字，CLIP计算的两个向量的距离是CLIP Score

好的，我们来将这份关于扩散模型（Diffusion Models）的笔记进行一次极其详尽、结构化且深入的翻译与扩展讲解。这份文档将从其哲学思想出发，深入剖析其工作原理、关键组件、与文本到图像生成任务的结合，并详细介绍Stable Diffusion、CLIP等核心技术。

---

### **扩散模型（Diffusion Models）：从混沌中雕琢秩序的艺术**

扩散模型是近年来生成模型领域的巅峰之作，它以其无与伦比的生成质量和稳定性，成为了Stable Diffusion、Midjourney、DALL-E 2/3等顶级文生图模型的基石。

#### **第一部分：扩散模型的核心哲学与工作原理**

笔记开头的米开朗基罗名言，完美地概括了扩散模型的哲学思想：

> **“在动手之前，完美的雕像早已存在于大理石之中。它就在那里，我需要做的，仅仅是凿去那些多余的部分。” — 米开朗基罗**

*   **扩散模型的类比**:
    *   **纯噪声 (Pure Noise)**: 就像那块未经雕琢的“大理石”，它包含了无穷的可能性。
    *   **去噪过程 (Denoising Process)**: 就像米开朗基罗“凿去多余部分”的过程。模型不是在凭空创造，而是在一个混沌的整体中，逐步、精细地**移除“噪声”（多余部分）**，最终让内在的“雕像”（清晰的图像）显现出来。

##### **1. 反向过程 (Reverse Process)：生成图像的“雕刻”之旅**

这是我们实际使用扩散模型生成图片时发生的过程。

1.  **起点**: 从一个与我们期望的图片维度完全一致的、完全由**标准正态分布**采样的随机噪声向量（即一张纯噪声图像）开始。
2.  **迭代去噪**: 将这张噪声图像输入到一个核心的**去噪模型 (Denoise Model)** 中。这个模型会进行数百上千次的迭代。
3.  **共享权重与时间步**: 在这上千次迭代中，使用的是**同一个去噪模型**。但为了让模型知道当前处于“雕刻”的哪个阶段（是早期的大刀阔斧，还是晚期的精雕细琢），每次迭代都会将一个代表当前**时间步 (Time Step)** 的编码输入给模型。Time Step就像一个进度条，告诉模型当前的噪声程度有多大。
4.  **终点**: 经过所有步骤的迭代去噪后，纯噪声图像最终会变成一张清晰、高质量的图像。

##### **2. 去噪模块的设计：预测噪声，而非直接去噪**

一个关键的设计选择是，去噪模型预测的**不是**“去噪后的干净图像”，而是“**当前图像中包含的噪声**”。

*   **工作流程**:
    1.  **输入**: 带有噪声的图片 + 当前的时间步 Time Step。
    2.  **输出**: 模型预测出的、添加到图片中的**噪声本身**。
    3.  **更新**: 用输入的带噪图片，**减去**模型预测出的噪声，得到一个更干净一点的图片，作为下一步迭代的输入。
*   **为什么这样做？**: 实验和理论都表明，直接预测一个结构化、高熵的“干净图像”是一个非常困难的目标。相比之下，预测非结构化、更简单的“噪声”是一个定义更明确、更容易学习的任务，这使得训练过程更加稳定和高效。

##### **3. 训练过程：前向过程 (Forward Process) 的“人工数据制造”**

我们如何获得训练去噪模型所需的大量“带噪图片-噪声”数据对呢？答案是通过一个人工设计的**前向过程（也叫扩散过程）**。

1.  **起点**: 从一张真实的、干净的训练图片开始（例如，一张猫的图片）。
2.  **逐步加噪**: 定义一个固定的、逐步的加噪流程。在数百个时间步中，一步一步地向这张干净图片中添加少量的高斯噪声。
3.  **终点**: 经过所有步骤，原始图片最终会变成一张纯粹的随机噪声。
4.  **制造训练数据**: 这个加噪过程中的**每一步**，都为我们提供了一个完美的训练样本：
    *   **模型输入**: 当前步骤的带噪图片 + 当前的时间步。
    *   **真实标签 (Ground Truth)**: 在这一步我们**亲手加入**的那个噪声。

通过这个过程，我们可以从一张干净图片中，创造出数百个训练数据对，从而让模型学会从任意噪声程度的图片中精确地预测出噪声。

---

#### **第二部分：文本到图像的进化 (Text-to-Image Generation)**

要让扩散模型听懂人类的语言，我们需要引入文本信息。

1.  **数据需求**: 需要大量的**“图像-文本”成对数据**。这些数据告诉模型，什么样的文字描述对应什么样的视觉内容。LAION-5B (58.5亿对) 就是目前最大、最常用的开源数据集。
2.  **模型改造**: 在原有的去噪模型（Noise Predictor）中，再增加一个**文本输入**。现在，模型的输入变成了三个：
    *   带噪的图片
    *   当前的时间步 Time Step
    *   描述图像的**文本编码 (Text Embedding)**

模型的目标变成了：在**遵循文本描述的指导下**，预测出当前图像中的噪声。

---

#### **第三部分：Stable Diffusion 的架构揭秘**

Stable Diffusion之所以“Stable”且高效，关键在于它没有直接在昂贵的像素空间（Pixel Space）上进行扩散，而是在一个低维的**隐空间（Latent Space）**中进行。

其架构主要包含三大核心组件：

##### **1. 文本编码器 (Text Encoder) - 语言的理解者**

*   **作用**: 将输入的文本提示（Prompt）转换成一个机器能理解的、富含语义的数字向量（Embedding）。
*   **实现**: 通常使用一个预训练好的、强大的**CLIP Text Encoder**。
*   **关键洞察**: **文本编码器的质量和规模，极大地影响着生成图像的质量**，以及模型对Prompt的理解深度。一个更大的文本编码器，能让模型更好地理解复杂的、抽象的概念，其FID分数（评估指标）也越低。

##### **2. 生成模型 (U-Net) - 在隐空间中去噪**

这是Stable Diffusion的核心，一个在**隐空间**中工作的噪声预测器（通常是U-Net结构）。

*   **训练过程**:
    1.  **加噪对象**: 噪声不是直接加在原始高清图片上，而是加在由一个预训练好的**Encoder**压缩后得到的**隐空间表示（Latent Representation）**上。这个隐空间表示可以看作是原始图片的一个“语义压缩包”，维度远小于原始图片（例如，从 512x512x3 压缩到 64x64x4）。
    2.  **模型输入**:
        *   加了噪的隐空间表示
        *   时间步 Time Step
        *   文本编码器输出的文本向量
    3.  **模型输出**: 预测出的、添加到隐空间表示中的**噪声**。

*   **优势**: 在低维的隐空间中进行去噪，计算成本相比在像素空间中**指数级下降**，使得模型可以在消费级硬件上快速运行。

##### **3. 图像解码器 (Image Decoder) - 视觉的绘制者**

*   **作用**: 将在隐空间中经过千锤百炼、最终去噪完成的那个“干净的”隐空间表示，**解码、放大、还原**成一张完整的高清像素图像。
*   **实现**: 它来自于一个预训练好的**变分自编码器（VAE）**的Decoder部分。这个VAE被训练用于将高清图片编码到隐空间，再从隐空间解码回高清图片。训练完成后，我们只取其Decoder部分来使用。
*   **关键洞察**: 实验发现，图像解码器的规模对最终生成质量影响相对较小。一个好的解码器能保证图像细节，但生成内容的核心质量和语义一致性，主要由在隐空间中工作的U-Net和文本编码器决定。

---

#### **第四部分：关键的评估指标与支撑技术**

##### **1. FID (Fréchet Inception Distance) 分数 - 图像质量的“黄金标准”**

FID是目前评估生成模型图像质量最主流、最可靠的指标。

*   **计算过程**:
    1.  使用一个在ImageNet上预训练好的**InceptionV3**网络作为特征提取器。
    2.  将一批**真实图片**和一批**生成图片**分别输入该网络，得到它们在网络深层的**隐空间表示（Latent Representations）**。
    3.  假设这两组隐空间表示分别服从两个多维高斯分布。
    4.  计算这两个高斯分布之间的**弗雷歇距离（Fréchet Distance）**。这个距离同时考虑了两个分布的均值和协方差，既能衡量生成图像的保真度（是否像真图），也能衡量其多样性（是否覆盖了真实图像的各种模式）。
*   **解读**: **FID分数越低越好**，代表生成图像的分布与真实图像的分布越接近。计算FID需要采样大量的图片（通常是1万张或5万张）才能得到稳定、有意义的结果。

##### **2. CLIP (Contrastive Language-Image Pre-Training) - 连接文本与图像的桥梁**

CLIP是文生图领域最重要的基础模型之一。

*   **核心思想**: 通过**对比学习**，在一个巨大的（4亿对）“图像-文本”数据集上，共同训练一个**图像编码器 (Image Encoder)** 和一个**文本编码器 (Text Encoder)**。
*   **训练目标**:
    *   对于一个**成对**的图像和文本，它们经过各自编码器后得到的向量（Embedding）在特征空间中应该尽可能**接近**。
    *   对于一个**不成对**的图像和文本，它们的向量应该尽可能**远离**。
*   **能力**: 训练完成后，CLIP的编码器获得了一种强大的**跨模态理解能力**。它知道“狗”的文字和狗的图片在语义上是相关的。
*   **CLIP Score**: 一种简单的评估指标。将生成的图片和输入的文本提示分别输入CLIP的编码器，计算它们向量之间的**余弦相似度**。相似度越高，说明生成的图片在语义上与文本提示越匹配。但它只衡量语义一致性，不衡量图像本身的真实感。