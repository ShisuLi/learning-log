
# 梯度下降 (Gradient Descent) 学习笔记

## 1. 核心思想：下山最快

梯度下降是一种应用广泛的**优化算法**，其核心目标是找到函数的**最小值**。

想象一个场景：你站在一座山脉的某一点，浓雾弥漫，看不清山谷（最低点）在哪里。为了尽快下到山谷，你该怎么办？

最直观的方法是：
1.  环顾四周，找到当前位置**最陡峭的下坡方向**。
2.  朝着这个方向走一小步。
3.  到达新位置后，重复第一步和第二步。
4.  不断重复，直到你感觉自己已经到达了某个最低点（再走就该上坡了）。

**梯度下降就是这个过程的数学化**：
- **山脉**：就是我们要优化的**损失函数 (Loss Function)** 或成本函数 (Cost Function)，记为 $J(\theta)$。
- **你的位置**：就是当前模型的**参数 (Parameters)**，记为 $\theta$。
- **最陡峭的下坡方向**：就是损失函数在当前位置的**梯度的相反方向**，即 $-\nabla J(\theta)$。
- **你走的一小步**：就是**学习率 (Learning Rate)**，记为 $\eta$ (或 $\alpha$)，它控制了每一步的步长。

---

## 2. 数学原理

### 2.1 梯度 (Gradient)

梯度是一个向量，指向函数在某一点**增长最快**的方向。对于一个有多个参数 $\theta = (\theta_0, \theta_1, ..., \theta_n)$ 的函数 $J(\theta)$，其梯度被定义为各个参数的偏导数构成的向量：

$$
\nabla J(\theta) = \begin{pmatrix} \frac{\partial J}{\partial \theta_0} \\ \frac{\partial J}{\partial \theta_1} \\ \vdots \\ \frac{\partial J}{\partial \theta_n} \end{pmatrix}
$$

既然梯度指向增长最快的方向，那么**梯度的相反方向** (`-∇J(θ)`) 就是**下降最快**的方向。

### 2.2 更新规则 (Update Rule)

梯度下降算法的迭代更新规则非常简洁：

$$
\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla J(\theta_{\text{old}})
$$

- $\theta_{\text{new}}$：更新后的参数值。
- $\theta_{\text{old}}$：更新前的参数值。
- $\eta$ (Eta)：学习率，一个超参数，决定了每次更新的步幅大小。
- $\nabla J(\theta_{\text{old}})$：在旧参数位置计算出的梯度。

这个过程会一直迭代，直到梯度接近于零，或者损失函数的值不再显著下降，我们就认为找到了一个（局部）最小值。

---

## 3. 梯度下降的三个变种

根据每次更新参数时使用的**数据量**不同，梯度下降可以分为三种主要类型。假设我们有 $m$ 个训练样本。

### 3.1 批量梯度下降 (Batch Gradient Descent, BGD)

**特点**：在每次更新参数时，使用**所有**的训练数据来计算梯度。

- **更新规则**：
  $$
  \theta = \theta - \eta \nabla J_{\text{all}}(\theta)
  $$
- **优点**：
    1.  **方向准确**：由于使用了全部数据，每次更新都朝着全局最优解的方向前进，路径平滑稳定。
    2.  **易于并行**：可以利用向量化计算，一次性处理所有数据。
- **缺点**：
    1.  **计算成本高**：当数据集非常大时（例如百万级样本），每次迭代都要计算所有样本的梯度，速度非常慢，内存消耗也大。
    2.  **不适合在线学习**：无法在新数据到来时实时更新模型。

### 3.2 随机梯度下降 (Stochastic Gradient Descent, SGD)

**特点**：在每次更新参数时，**随机选择一个**训练样本来计算梯度。

- **更新规则**：
  $$
  \text{For } i \text{ in } 1, ..., m: \\
  \theta = \theta - \eta \nabla J_{i}(\theta)
  $$
- **优点**：
    1.  **更新速度快**：每次迭代只用一个样本，计算成本极低。
    2.  **适合在线学习**：可以随时根据新样本更新模型。
    3.  **随机性有助于跳出局部最优**：更新路径非常嘈杂（像醉汉下山），这种随机性有时能帮助算法跳出较差的局部最小值，寻找到更好的解。
- **缺点**：
    1.  **收敛不稳定**：损失函数下降的路径非常曲折，震荡剧烈，难以精确收敛到最小值。
    2.  **难以充分利用向量化**：因为一次只处理一个样本。

### 3.3 小批量梯度下降 (Mini-Batch Gradient Descent, MBGD)

**特点**：这是 BGD 和 SGD 的折衷方案。每次更新时，使用一小批（mini-batch）训练样本（例如 32, 64, 128 个）来计算梯度。

- **更新规则**：
  $$
  \text{For batch } b \text{ in all batches}: \\
  \theta = \theta - \eta \nabla J_{b}(\theta)
  $$
- **优点**：
    1.  **兼顾了速度与稳定性**：比 BGD 快，比 SGD 稳定。
    2.  **充分利用硬件**：可以高效地进行向量化计算，充分利用 CPU 和 GPU 的性能。
- **缺点**：
    1.  增加了一个超参数：批次大小 (batch size)。

**总结**：在现代深度学习应用中，**小批量梯度下降 (MBGD) 是最常用、最主流的优化方法**。

| 特性 | 批量梯度下降 (BGD) | 随机梯度下降 (SGD) | 小批量梯度下降 (MBGD) |
| :--- | :--- | :--- | :--- |
| **每次迭代数据量** | 全部数据集 | 1个样本 | 一个批次 (e.g., 32-256) |
| **更新速度** | 慢 | 快 | 中等 |
| **内存占用** | 高 | 低 | 中等 |
| **收敛路径** | 平滑，直指最小值 | 嘈杂，剧烈震荡 | 相对平滑，略有震荡 |
| **应用场景** | 小数据集 | 在线学习，大数据集 | **主流选择** |

---

## 4. 挑战与优化

传统的梯度下降面临一些挑战，催生了许多先进的优化算法。

### 4.1 挑战

1.  **选择合适的学习率 $\eta$**：
    - **太小**：收敛速度过慢。
    - **太大**：可能导致损失函数在最小值附近剧烈震荡，甚至发散（不收敛）。
2.  **局部最小值 (Local Minima)**：梯度下降容易陷入局部最小值，而无法找到全局最小值。
3.  **鞍点 (Saddle Points)**：在高维空间中，鞍点（一个方向是极大值，另一方向是极小值）比局部最小值更常见。在鞍点处梯度也为零，会导致算法停滞。

### 4.2 优化算法

为了应对这些挑战，研究者们提出了多种自适应学习率的优化算法。它们的核心思想是为每个参数自动调整学习率。

- **Momentum (动量法)**：
    - 引入了“动量”的概念，模拟物体运动的惯性。
    - 每次更新不仅考虑当前梯度，还累积了历史的更新方向。
    - **效果**：有助于加速通过梯度平缓的区域，并抑制更新方向的震荡，使其更快地收敛。

- **Adagrad (Adaptive Gradient)**：
    - 为**不同参数**自适应地调整学习率。
    - 对**更新频繁**的参数，给予**较小**的学习率；对**更新稀疏**的参数，给予**较大**的学习率。
    - **效果**：非常适合处理稀疏数据（如 NLP 中的词嵌入）。但其学习率会随时间单调递减，可能导致后期训练过早停止。

- **RMSprop (Root Mean Square Propagation)**：
    - Adagrad 的改进版，通过引入一个衰减因子来解决学习率过早衰减的问题。
    - 它只累积最近一段时间的梯度信息，而不是全部历史梯度。

- **Adam (Adaptive Moment Estimation)**：
    - **目前最流行、最常用的优化器之一**。
    - 它**结合了 Momentum 和 RMSprop 的优点**：既利用动量来加速收敛，又利用自适应学习率来为每个参数调整步幅。
    - 通常在各种任务中都有着稳定而出色的表现。

在实践中，通常会**首选 Adam** 作为优化器，因为它在大多数情况下都能取得良好的效果。


# 优化器算法推导笔记：Adagrad, RMSprop, Adam

## 0. 符号约定

- $J(\theta)$: 损失函数，$\theta$ 是模型参数。
- $g_t = \nabla_\theta J(\theta_t)$: 在第 $t$ 次迭代时，损失函数对参数 $\theta$ 的梯度。
- $\theta_{t+1}$: 第 $t+1$ 次迭代更新后的参数。
- $\eta$: 全局学习率 (Learning Rate)。
- $\odot$: 逐元素乘法 (Hadamard Product)。
- $\oslash$: 逐元素除法。
- $\epsilon$: 一个非常小的正数（通常为 $10^{-8}$），用于防止分母为零。

---

## 1. Adagrad (Adaptive Gradient Algorithm)

**核心思想**：为模型中的**每一个参数**自适应地计算不同的学习率。具体来说，对于**更新频繁**（梯度累积较大）的参数，其学习率会**衰减得更快**；对于**更新稀疏**（梯度累积较小）的参数，其学习率会**衰减得更慢**。

### 推导过程

1.  **定义梯度平方的累积量 $G_t$**：
    在每次迭代 $t$ 时，我们定义一个累积变量 $G_t$，它存储了从开始到当前时刻**所有梯度值的平方和**。这是一个与参数 $\theta$ 维度相同的向量或矩阵。

    $$
    G_t = G_{t-1} + g_t \odot g_t
    $$

    -   $G_0$ 初始化为零向量。
    -   $g_t \odot g_t$ 表示将当前梯度向量 $g_t$ 的每个元素进行平方。
    -   这个公式意味着 $G_t$ 会不断累积，其值会单调递增。
        $$
        G_t = \sum_{i=1}^{t} g_i \odot g_i
        $$

2.  **参数更新规则**：
    Adagrad 的参数更新规则修改了标准梯度下降，它将全局学习率 $\eta$ 除以一个与 $G_t$ 相关的项。

    $$
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t
    $$

    -   $\sqrt{G_t + \epsilon}$：对累积量 $G_t$ 的每个元素开平方根（并加上 $\epsilon$ 防止分母为零）。
    -   $\frac{\eta}{\sqrt{G_t + \epsilon}}$：这就是**针对每个参数的自适应学习率**。
        -   如果某个参数的梯度从始至终都很大，其在 $G_t$ 中对应的元素值就会很大，导致其有效学习率 $\frac{\eta}{\sqrt{G_t}}$ 变得很小。
        -   反之，如果某个参数的梯度很小，其有效学习率会保持在较大的水平。

### 优缺点

-   **优点**：对于处理稀疏数据（如自然语言处理中的词嵌入）非常有效。
-   **缺点**：由于 $G_t$ 是单调递增的，学习率会随着训练的进行持续下降，最终可能变得过小，导致模型在训练后期学习能力几乎停滞，无法收敛到最优解。

---

## 2. RMSprop (Root Mean Square Propagation)

**核心思想**：解决 Adagrad 学习率过早衰减的问题。它不再累积**所有**历史梯度，而是只关注**最近一段时间**的梯度信息，通过引入一个衰减率 $\beta$ (beta) 来实现这一点。

### 推导过程

1.  **定义梯度平方的指数移动平均 $S_t$**：
    RMSprop 不再使用简单的累加，而是使用**指数移动平均 (Exponential Moving Average, EMA)** 来计算梯度平方的“近期平均值”。

    $$
    S_t = \beta S_{t-1} + (1 - \beta) (g_t \odot g_t)
    $$

    -   $S_0$ 初始化为零向量。
    -   $\beta$ 是衰减率或动量因子，通常取值为 `0.9` 或 `0.99`。
    -   这个公式的含义是：当前的平均值 $S_t$ 是由 $90\%$ 的**上一时刻平均值 $S_{t-1}$** 和 $10\%$ 的**当前梯度平方 $g_t \odot g_t$** 加权平均得到的。
    -   这种机制使得 오래된梯度信息的影响会随着时间呈指数级衰减，而近期梯度信息占主导地位。

2.  **参数更新规则**：
    更新规则与 Adagrad 非常相似，只是将累积量 $G_t$ 换成了指数移动平均量 $S_t$。

    $$
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{S_t + \epsilon}} \odot g_t
    $$

### 优缺点

-   **优点**：解决了 Adagrad 学习率消失的问题，在非凸优化问题中表现通常比 Adagrad 更好。
-   **缺点**：仍然只考虑了梯度的二阶矩（梯度平方的平均），没有结合一阶矩（梯度本身）。

---

## 3. Adam (Adaptive Moment Estimation)

**核心思想**：**集大成者**。它同时利用了梯度的一阶矩估计（梯度的平均值，类似 Momentum）和二阶矩估计（梯度平方的平均值，类似 RMSprop），并对这两个估计值进行了偏差修正。

### 推导过程

1.  **计算梯度的一阶矩估计 (动量) $m_t$**：
    这部分与 Momentum 算法类似，使用指数移动平均来估计梯度的均值。

    $$
    m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
    $$

    -   $m_0$ 初始化为零向量。
    -   $\beta_1$ 是一阶矩的衰减率，通常取值为 `0.9`。
    -   $m_t$ 可以看作是“动量”，它累积了过去梯度的方向。

2.  **计算梯度的二阶矩估计 (梯度平方) $v_t$**：
    这部分与 RMSprop 完全相同，使用指数移动平均来估计梯度平方的均值。

    $$
    v_t = \beta_2 v_{t-1} + (1 - \beta_2) (g_t \odot g_t)
    $$

    -   $v_0$ 初始化为零向量。
    -   $\beta_2$ 是二阶矩的衰减率，通常取值为 `0.999`。
    -   $v_t$ 用于自适应地调整每个参数的学习率。

3.  **偏差修正 (Bias Correction)**：
    由于 $m_0$ 和 $v_0$ 都被初始化为零，在训练初期，$m_t$ 和 $v_t$ 的值会比真实均值更接近于零。为了修正这个偏差，Adam 引入了修正步骤：

    $$
    \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
    $$
    $$
    \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    $$

    -   $t$ 是当前的迭代次数。
    -   在初期，$1 - \beta^t$ 的值很小，这会放大 $m_t$ 和 $v_t$，使其接近真实值。随着 $t$ 增大，$\beta^t$ 趋近于零，这个修正项就几乎不起作用了。

4.  **最终参数更新规则**：
    Adam 结合了修正后的一阶矩和二阶矩来更新参数。

    $$
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
    $$

    -   $\hat{m}_t$：提供了更新的**方向和动量**。
    -   $\sqrt{\hat{v}_t} + \epsilon$：提供了**自适应的学习率缩放**，对不同参数进行调整。

### 优缺点

-   **优点**：
    1.  结合了 Momentum 和 RMSprop 的优点。
    2.  对每个参数都有自适应的学习率。
    3.  在各种不同的问题和模型上都表现出色且稳定。
    4.  超参数具有很好的解释性，且通常不需要怎么调整。
-   **缺点**：虽然非常罕见，但在某些特定问题上，Adam 可能无法收敛到最优解。

**总结**：Adam 因其强大的性能和鲁棒性，已成为深度学习领域中最常用、最值得信赖的默认优化器之一。
